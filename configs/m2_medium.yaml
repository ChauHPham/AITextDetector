# Optimized config for M2 Mac with 10k-50k samples
# Training time: ~30-90 minutes
# RECOMMENDED for best balance
data_path: data/dataset.csv
base_model: roberta-base
save_dir: models/ai_detector

max_length: 256
batch_size: 8   # Standard batch size
num_epochs: 2   # 2 epochs usually enough
lr: 5e-5
weight_decay: 0.01
logging_steps: 50
eval_strategy: epoch
seed: 42
gradient_accumulation_steps: 2  # Effective batch size = 16
fp16: false  # M2 Mac doesn't have CUDA
warmup_ratio: 0.1
save_total_limit: 2
save_steps: 0
dataloader_num_workers: 0  # macOS requires 0 to avoid threading issues
